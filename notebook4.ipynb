{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory on LLM Chain\n",
    "\n",
    "LLM Chain is off-the-shelf chain, which is for general purpose.\n",
    "Customed Chain is more preferred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "Human: My name is Minji\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you, Minji! How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.9,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=50,\n",
    "    return_messages=True, # False: Just String. True: return as Message Class\n",
    "    memory_key=\"chat_history\" # To connect memory with Template->Prompt->Chain.\n",
    ")\n",
    "\n",
    "# template = \"\"\"\n",
    "#     You are a kind AI talking to a human\n",
    "\n",
    "#     {chat_history}\n",
    "#     Human: {question}\n",
    "#     You: \n",
    "# \"\"\"\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a kind AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # unlimited amount of messages can be stored\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True # For debug - Prompt log\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Minji\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "Human: My name is Minji\n",
      "AI: It's nice to meet you, Minji! How can I assist you today?\n",
      "Human: I live in a Halloween Town\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That sounds like a fun and festive place to live! Do you have any exciting plans for Halloween this year in your Halloween Town?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in a Halloween Town\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "System: The human introduces themselves as Minji. The AI responds warmly and asks how it can assist Minji today.\n",
      "Human: I live in a Halloween Town\n",
      "AI: That sounds like a fun and festive place to live! Do you have any exciting plans for Halloween this year in your Halloween Town?\n",
      "Human: It is very cozy like sleeping in a coffin.. do you remember where am I living?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you mentioned earlier that you live in a Halloween Town. It sounds like a unique and cozy place, similar to the feeling of sleeping in a coffin! Is there anything specific you enjoy about living in this Halloween-themed environment?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"It is very cozy like sleeping in a coffin.. do you remember where am I living?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory on Customed Chain\n",
    "Prompt and memories are same.\n",
    "\n",
    "RunnablePassthrough - the class allows you to run the function before the prompt is formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'My name is Minji'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Minji, it's nice to meet you. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 25, 'total_tokens': 43}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d96a3773-7573-480e-9175-2def8f3a1933-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.9\n",
    "    )\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=50,\n",
    "    return_messages=True, # False: Just String. True: return as Message Class\n",
    "    memory_key=\"chat_history\" # To connect memory with Template->Prompt->Chain.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a kind AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # unlimited amount of messages can be stored\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm = llm,\n",
    "#     memory = memory,\n",
    "#     prompt = prompt,\n",
    "#     verbose = True #  Prompt log\n",
    "# )\n",
    "\n",
    "## Without runnable pass through\n",
    "# chain = prompt | llm\n",
    "# chain.invoke({ # chain호출\n",
    "#     \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"], // This can be conplicated\n",
    "#     \"question\": \"My name is Minji\"\n",
    "# })\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input) # question\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "chain.invoke({ # chain호출\n",
    "    \"question\": \"My name is Minji\"\n",
    "})# first input of the chain: dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
