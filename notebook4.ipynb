{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory on LLM Chain\n",
    "\n",
    "LLM Chain is off-the-shelf chain, which is for general purpose.\n",
    "Customed Chain is more preferred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "Human: My name is Minji\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you, Minji! How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.9,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=50,\n",
    "    return_messages=True, # False: Just String. True: return as Message Class\n",
    "    memory_key=\"chat_history\" # To connect memory with Template->Prompt->Chain.\n",
    ")\n",
    "\n",
    "# template = \"\"\"\n",
    "#     You are a kind AI talking to a human\n",
    "\n",
    "#     {chat_history}\n",
    "#     Human: {question}\n",
    "#     You: \n",
    "# \"\"\"\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a kind AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # unlimited amount of messages can be stored\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    verbose = True # For debug - Prompt log\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Minji\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "Human: My name is Minji\n",
      "AI: It's nice to meet you, Minji! How can I assist you today?\n",
      "Human: I live in a Halloween Town\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That sounds like a fun and festive place to live! Do you have any exciting plans for Halloween this year in your Halloween Town?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in a Halloween Town\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a kind AI talking to a human\n",
      "System: The human introduces themselves as Minji. The AI responds warmly and asks how it can assist Minji today.\n",
      "Human: I live in a Halloween Town\n",
      "AI: That sounds like a fun and festive place to live! Do you have any exciting plans for Halloween this year in your Halloween Town?\n",
      "Human: It is very cozy like sleeping in a coffin.. do you remember where am I living?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you mentioned earlier that you live in a Halloween Town. It sounds like a unique and cozy place, similar to the feeling of sleeping in a coffin! Is there anything specific you enjoy about living in this Halloween-themed environment?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"It is very cozy like sleeping in a coffin.. do you remember where am I living?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory on Customed Chain\n",
    "Prompt and memories are same.\n",
    "\n",
    "RunnablePassthrough - the class allows you to run the function before the prompt is formatted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '\\n             I am coughing from few hours ago.\\n             I took good care of myself, but it was an environment where colds were bound to spread.\\n             how can I recover fast in this case?\\n             '}\n",
      "content=\"I'm sorry to hear that you're not feeling well. Here are some tips to help you recover from a cold faster:\\n\\n1. Stay hydrated: Drink plenty of fluids such as water, herbal tea, or broth to help loosen congestion and prevent dehydration.\\n\\n2. Rest: Make sure to get plenty of rest to give your body time to recover. Listen to your body and take it easy.\\n\\n3. Stay warm: Keep yourself warm to help your body fight off the cold virus more effectively.\\n\\n4. Gargle with warm salt water: Gargling with warm salt water can help soothe a sore throat and reduce inflammation.\\n\\n5. Use a humidifier: Using a humidifier in your room can help keep your nasal passages moist and reduce congestion.\\n\\n6. Eat healthy foods: Try to eat nutritious foods like fruits, vegetables, and whole grains to support your immune system.\\n\\n7. Consider over-the-counter remedies: You can try over-the-counter cold medications such as decongestants, antihistamines, or pain relievers to help alleviate your symptoms.\\n\\nIf your symptoms persist or worsen, it's important to consult with a healthcare professional for further evaluation and treatment. Take care and I hope you feel better soon!\" response_metadata={'token_usage': {'completion_tokens': 248, 'prompt_tokens': 63, 'total_tokens': 311}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d35dece7-b995-435e-a8cf-1fc7bea1119d-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.9\n",
    "    )\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=50,\n",
    "    return_messages=True, # False: Just String. True: return as Message Class\n",
    "    memory_key=\"history\" # To connect memory with Template->Prompt->Chain.\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a kind AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # unlimited amount of messages can be stored\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm = llm,\n",
    "#     memory = memory,\n",
    "#     prompt = prompt,\n",
    "#     verbose = True #  Prompt log\n",
    "# )\n",
    "\n",
    "## Without runnable pass through\n",
    "# chain = prompt | llm\n",
    "# chain.invoke({ # chain호출\n",
    "#     \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"], // This can be conplicated\n",
    "#     \"question\": \"My name is Minji\"\n",
    "# })\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input) # question\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm # chain runs load memory method, and the return will be passed to the prompt with the name 'chat_history', with 'question'.\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({ # chain호출\n",
    "        \"question\": question\n",
    "    })# first input of the chain: dictionary\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content}) \n",
    "    # save output of the memory.\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"\"\"\n",
    "             I am coughing from few hours ago.\n",
    "             I took good care of myself, but it was an environment where colds were bound to spread.\n",
    "             how can I recover fast in this case?\n",
    "             \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
